

| id   | 论文                                                         | 学校         | 水下数据集                                               |
| ---- | ------------------------------------------------------------ | ------------ | -------------------------------------------------------- |
| 1    | [基于深度学习的ROV水下目标检测方法研究_魏铭](C:\Users\zhang\Desktop\硕士文献\基于深度学习的ROV水下目标检测方法研究_魏铭.pdf) | 苏州科技大学 | URPC 水产品数据集                                        |
| 2    | [基于深度学习的海底水下目标检测研究_叶赵兵](C:\Users\zhang\Desktop\硕士文献\基于深度学习的海底水下目标检测研究_叶赵兵.pdf) | 苏州科技大学 | 2021 年全国水下机器人大赛数据集                          |
| 3    | [基于深度学习的水面漂浮物检测方法研究_翁云龙](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水面漂浮物检测方法研究_翁云龙.pdf) | 浙江科技     | 自己采集                                                 |
| 4    | [基于深度学习的水下图像增强和水下目标检测技术研究_崔博文](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下图像增强和水下目标检测技术研究_崔博文.pdf) | 江苏科技     | 1 2022全国水下机器人大赛数据集  2 Trash_ICRA19 数据集    |
| 5    | [基于深度学习的水下图像增强与目标检测_马敏慧](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下图像增强与目标检测_马敏慧.pdf) | 江苏科技     | EUVP 数据集                                              |
| 6    | [基于深度CNN的海洋生物目标检测算法研究](C:\Users\zhang\Desktop\硕士文献\基于深度CNN的海洋生物目标检测算法研究_张健.pdf) | 桂林电子     | 2021 年全国水下机器人大赛-水下光学图像目标检测算法比赛， |
| 7    | [基于深度学习的水下目标检测方法研究_邱峰](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下目标检测方法研究_邱峰.pdf) | 桂林电子     | Fish4Knowledge 鱼类数据集                                |
| 8    | [基于深度学习的水下黄鱼智能监测系统的研究](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下黄鱼智能监测系统的研究_丁梦雅.pdf) | 浙大         | 自己采集                                                 |
| 9    | [基于域自适应的水下目标检测方法研究_苏知青](C:\Users\zhang\Desktop\硕士文献\基于域自适应的水下目标检测方法研究_苏知青.pdf) |              | URPC                                                     |
| 10   | [基于自注意力机制的水下目标智能跟踪与预测算法研究_王帅](C:\Users\zhang\Desktop\硕士文献\基于自注意力机制的水下目标智能跟踪与预测算法研究_王帅.pdf) |              | 自制的水下多目标跟踪数据集 UMOT                          |
| 11   | [基于深度学习的水下目标检测算法研究_谢晓东](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下目标检测算法研究_谢晓东.pdf) |              | DUO 数据集                                               |
| 12   | [基于深度学习的水下目标检测算法研究_温个](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下目标检测算法研究_温个.pdf) |              | 2019 年中国水下机器人大赛                                |
| 13   | [基于CenterNet的水下目标检测方法研究_于瀛](C:\Users\zhang\Desktop\硕士文献\基于CenterNet的水下目标检测方法研究_于瀛.pdf) |              | URPC 水下机器人捕捉竞赛的水下检测数据集                  |
| 14   | [基于YOLOv5的海洋生物目标检测算法研究_朱伟东](C:\Users\zhang\Desktop\硕士文献\基于YOLOv5的海洋生物目标检测算法研究_朱伟东.pdf) |              | 全国水下机器人大赛提供的数据集                           |
| 15   | [基于改进YOLOv5的水下群体目标检测研究与实现_李海清](C:\Users\zhang\Desktop\硕士文献\基于改进YOLOv5的水下群体目标检测研究与实现_李海清.pdf) |              | 自己采集                                                 |
|      |                                                              |              |                                                          |
|      |                                                              |              |                                                          |
|      |                                                              |              |                                                          |

[文献参考](https://zhuanlan.zhihu.com/p/648284422)

![image-20240528220336843](https://cdn.jsdelivr.net/gh/ZhangYuQiao326/study_nodes_pictures@main/img/image-20240528220336843.png)

| id   | 论文                                                         | 模型                                                         |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | [基于深度学习的ROV水下目标检测方法研究_魏铭](C:\Users\zhang\Desktop\硕士文献\基于深度学习的ROV水下目标检测方法研究_魏铭.pdf) | YoLOv5+CA+解耦头+可变形卷积                                  |
| 2    | [基于深度学习的海底水下目标检测研究_叶赵兵](C:\Users\zhang\Desktop\硕士文献\基于深度学习的海底水下目标检测研究_叶赵兵.pdf) | YoLov5+CA+特征融合BiFpn(PANet)+Ghost卷积(普通卷积)+损失函数  |
| 3    | [基于深度学习的水面漂浮物检测方法研究_翁云龙](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水面漂浮物检测方法研究_翁云龙.pdf) | YoLov5s+ 多尺度检测 + SimAM无参注意力机制+CARAFE上采样+卷积混合层 |
| 4    | [基于深度学习的水下图像增强和水下目标检测技术研究_崔博文](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下图像增强和水下目标检测技术研究_崔博文.pdf) | 图像增强：生成对抗网络 + SeLU激活函数 + RepVGG网络 + L1损失                                   图像检测：Faster R-CNN + 特征提取ResNet-50(VGG-16) + CBAM注意力机制 + CIoU损失函数 |
| 5    | [基于深度学习的水下图像增强与目标检测_马敏慧](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下图像增强与目标检测_马敏慧.pdf) | 图像增强：=== 检测：RetinaNet + 特征提取 + 特征融合 + 特征检测 + 损失函数 |
| 6    | [基于深度CNN的海洋生物目标检测算法研究](C:\Users\zhang\Desktop\硕士文献\基于深度CNN的海洋生物目标检测算法研究_张健.pdf) | Faster R-CNN + 两个注意力机制 + ResNet50 + FPN               |
| 7    | [基于深度学习的水下目标检测方法研究_邱峰](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下目标检测方法研究_邱峰.pdf) |                                                              |
| 8    | [基于深度学习的水下黄鱼智能监测系统的研究](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下黄鱼智能监测系统的研究_丁梦雅.pdf) |                                                              |
| 9    | [基于域自适应的水下目标检测方法研究_苏知青](C:\Users\zhang\Desktop\硕士文献\基于域自适应的水下目标检测方法研究_苏知青.pdf) |                                                              |
| 10   | [基于自注意力机制的水下目标智能跟踪与预测算法研究_王帅](C:\Users\zhang\Desktop\硕士文献\基于自注意力机制的水下目标智能跟踪与预测算法研究_王帅.pdf) |                                                              |
| 11   | [基于深度学习的水下目标检测算法研究_谢晓东](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下目标检测算法研究_谢晓东.pdf) |                                                              |
| 12   | [基于深度学习的水下目标检测算法研究_温个](C:\Users\zhang\Desktop\硕士文献\基于深度学习的水下目标检测算法研究_温个.pdf) |                                                              |
| 13   | [基于CenterNet的水下目标检测方法研究_于瀛](C:\Users\zhang\Desktop\硕士文献\基于CenterNet的水下目标检测方法研究_于瀛.pdf) |                                                              |
| 14   | [基于YOLOv5的海洋生物目标检测算法研究_朱伟东](C:\Users\zhang\Desktop\硕士文献\基于YOLOv5的海洋生物目标检测算法研究_朱伟东.pdf) |                                                              |
| 15   | [基于改进YOLOv5的水下群体目标检测研究与实现_李海清](C:\Users\zhang\Desktop\硕士文献\基于改进YOLOv5的水下群体目标检测研究与实现_李海清.pdf) |                                                              |



水下图像处理方法：图像增强、图像复原

| 图像增强算法              | exp  |
| ------------------------- | ---- |
| Retinex 算法[             |      |
| 直方图均衡化算法          |      |
| 白平衡增强算法            |      |
| GLAD-NET 水下图像增强算法 |      |
| 生成对抗网络              |      |



| 单阶段：目标检测算法                     | 提到 | 年代 |
| ---------------------------------------- | ---- | ---- |
| 传统循环神经网络RNN                      | 1    |      |
| SSD                                      | 1    |      |
| YOLO，（效果不好，速度快，适合实时计算） | 1    | 2017 |
| Resnet                                   | 1    |      |

| 双阶段：目标检测算法                 |      |      |
| ------------------------------------ | ---- | ---- |
| R-CNN                                | 1    | 2016 |
| Fast R-CNN、                         | 5    |      |
| Faster R-CNN                         | 5    | 2015 |
| Mask R-CNN开源最新（效果好，速度慢） | 5    |      |
| 改进后的 Faster R-CNN                | 4    |      |

通道注意力、空间注意力和混合注意力

| 插件                     | exp                                         |
| ------------------------ | ------------------------------------------- |
| 注意力机制,              | SE  CBAM  CA SimAM SENet BAM SKNet          |
| 上采样操作（提高分辨率） | 邻近插值法、双线性插值法、反卷积法（3）     |
| 特征提取                 | VGG-16 ResNet-50（2）                       |
| 损失函数                 | IoU  CIoU（4） 修改loss func(5)             |
| 特征融合                 | PANet 、BiFpn（2）Dual-FPN（5）FPN(6)       |
| 特征检测                 | Anchor Based Anchor Free（5）               |
| 特征提取                 | ResNet-50 过拟合DenseNet-121（5）           |
| 卷积                     | 可学习分组卷积、普通卷积（5）Ghost卷积（2） |
| 网络                     |                                             |

# 基础模型

1 github搜关键词

2 检测：github 搜  openmmlab，第一个，总结了最新的模型

3 paper with code



# 开题

## 研究现状





修改：

题目： 基于TensorRT的xxx河流水下目标检测（不要优化）

工作量不够



1. 总数引用太多，现状分析不够，问题分析不够

   * 加入特殊场景，具体哪条河流

   * 存在什么问题，传统方法解决不了

     

2. 没有创新点。跟大数据结合

   * 应用大数据平台，自动采集数据
   * 对大数据进行处理，加入工作量
   * 具体模块算法进行改进，2个方法 + 1个系统
   * 

3. 系统设计
   * 加上软件设计的东西
4. 字体，图片，文献





1. 使用Hadoop或Spark等分布式计算框架来处理、存储和分析海量的水下图像和视频数据。
2. 分布式地执行数据增强操作，如图像翻转、亮度调整和合成生成等，从而大幅度扩展训练数据集。
3. 利用大数据技术中的实时数据流处理系统（Kafka、Flink），实现对实时视频数据流的快速处理和分析。



[UIEB主页](https://li-chongyi.github.io/proj_benchmark.html)

```py
# 离散小波变换（DWT）和逆离散小波变换（IDWT）：将输入图像分解为低频和高频分量，然后通过这些分量进行图像恢复。
import pywt
import torch
import torch.nn as nn

def dwt(x):
    """离散小波变换 (DWT)"""
    # 使用 Haar 小波对图像进行二维离散小波变换，输出四个分量：低频分量 (cA) 和三个高频分量 (cH, cV, cD)
    coeffs = pywt.dwt2(x.cpu().detach().numpy(), 'haar')
    cA, (cH, cV, cD) = coeffs
    return torch.tensor(cA).float(), torch.tensor(cH).float(), torch.tensor(cV).float(), torch.tensor(cD).float()

def idwt(cA, cH, cV, cD):
    """逆离散小波变换 (IDWT)"""
    # 使用 Haar 小波进行逆变换，将分解的分量重建为原始图像
    coeffs = (cA.cpu().detach().numpy(), (cH.cpu().detach().numpy(), cV.cpu().detach().numpy(), cD.cpu().detach().numpy()))
    return torch.tensor(pywt.idwt2(coeffs, 'haar')).float()

```



```py
# 颜色还原网络：使用 Swin Transformer 进行低频分量的处理，以恢复真实自然的颜色。
# 实现了 Swin Transformer 的主要操作，包括窗口注意力机制和 MLP 。
class SwinTransformerBlock(nn.Module):
    def __init__(self, in_channels, num_heads, window_size=4, shift_size=2):
        super(SwinTransformerBlock, self).__init__()
        self.window_size = window_size
        self.shift_size = shift_size
        
        # 标准化层，用于正则化数据
        self.norm1 = nn.LayerNorm(in_channels)
        # 多头注意力机制，聚合输入的特征
        self.attn = nn.MultiheadAttention(in_channels, num_heads)
        
        # 标准化层，用于正则化数据
        self.norm2 = nn.LayerNorm(in_channels)
        
        # 多层感知机，用于处理注意力后的特征
        self.mlp = nn.Sequential(
            nn.Linear(in_channels, in_channels * 4),
            nn.GELU(),
            nn.Linear(in_channels * 4, in_channels)
        )

    def forward(self, x):
        B, C, H, W = x.shape
        # 切片窗口
        x = x.view(B, C, -1).permute(2, 0, 1)  # 转换为 (H*W, B, C)
        x = self.norm1(x)
        attn_windows, _ = self.attn(x, x, x)
        x = x + attn_windows

        # 全局窗口移位
        x = x.permute(1, 2, 0).view(B, C, H, W)  # 转换回 (B, C, H, W)
        x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(2, 3))
        x = self.norm2(x)
        x = x + self.mlp(x)
        return x

class ColorRestorationNetwork(nn.Module):
    def __init__(self, in_channels, num_heads=8):
        super(ColorRestorationNetwork, self).__init__()
        
        # 卷积层，将图像嵌入到特定维度
        self.patch_embedding = nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=4, padding=0)
        
        # 多个 Swin Transformer 块，用于逐层处理嵌入特征
        self.swin_blocks = nn.Sequential(
            SwinTransformerBlock(in_channels, num_heads),
            SwinTransformerBlock(in_channels, num_heads),
            SwinTransformerBlock(in_channels, num_heads),
            SwinTransformerBlock(in_channels, num_heads)
        )
        
        # 反卷积层，用于将嵌入特征还原回原始分辨率。
        self.upsample = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=4, stride=4, padding=0)
        
    def forward(self, x):
        x = self.patch_embedding(x)
        x = self.swin_blocks(x)
        x = self.upsample(x)
        return x

```

```py
# 颜色还原网络，第二版--论文复原
import torch
import torch.nn as nn
from einops import rearrange
from torch.nn.functional import interpolate

# Swin Transformer Block
class SwinTransformerBlock(nn.Module):
    def __init__(self, in_channels, num_heads, window_size=7):
        super(SwinTransformerBlock, self).__init__()
        self.norm1 = nn.LayerNorm(in_channels)
        self.attn = nn.MultiheadAttention(embed_dim=in_channels, num_heads=num_heads)
        self.norm2 = nn.LayerNorm(in_channels)
        self.mlp = nn.Sequential(
            nn.Linear(in_channels, in_channels * 4),
            nn.GELU(),
            nn.Linear(in_channels * 4, in_channels)
        )
        self.window_size = window_size

    def forward(self, x):
        B, L, C = x.shape
        H = W = int(L ** 0.5)

        x = x.view(B, H, W, C)

        windows = x.unfold(1, self.window_size, self.window_size).unfold(2, self.window_size, self.window_size)
        windows = windows.contiguous().view(-1, self.window_size * self.window_size, C)

        attn_windows = self.attn(windows, windows, windows)[0]
        attn_windows = attn_windows.view(B, -1, H, W, C).permute(0, 1, 2, 3, 4).reshape(B, H * W, C)
        x = x + self.norm1(attn_windows)

        x = x + self.mlp(self.norm2(x))
        x = x.view(B, H, W, C)
        return x

# Patch Embedding
class PatchEmbedding(nn.Module):
    def __init__(self, in_channels, embed_dim):
        super(PatchEmbedding, self).__init__()
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=4, stride=4)
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        x = self.proj(x).flatten(2).transpose(1, 2)
        x = self.norm(x)
        return x

# Patch Merging (Downsampling)
class PatchMerging(nn.Module):
    def __init__(self, in_channels):
        super(PatchMerging, self).__init__()
        self.norm = nn.LayerNorm(in_channels)
        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)

    def forward(self, x):
        B, L, C = x.shape
        H = W = int(L ** 0.5)

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B, H/2, W/2, C
        x1 = x[:, 1::2, 0::2, :]  # B, H/2, W/2, C
        x2 = x[:, 0::2, 1::2, :]  # B, H/2, W/2, C
        x3 = x[:, 1::2, 1::2, :]  # B, H/2, W/2, C

        x = torch.cat([x0, x1, x2, x3], dim=-1)  # B, H/2, W/2, 4*C
        x = x.view(B, -1, 4 * C)

        x = self.norm(x)
        x = self.reduction(x)
        return x

# 上采样层
class UpsampleLayer(nn.Module):
    def __init__(self, in_channels, scale_factor=2):
        super(UpsampleLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, in_channels * scale_factor ** 2, kernel_size=3, padding=1)
        self.pixel_shuffle = nn.PixelShuffle(scale_factor)

    def forward(self, x):
        x = self.conv(x)
        x = self.pixel_shuffle(x)
        return x

# 颜色还原网络
class ColorRestorationNetwork(nn.Module):
    def __init__(self, in_channels=9, embed_dim=96, num_heads=8):
        super(ColorRestorationNetwork, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, 96, kernel_size=3, padding=1)

        # Patch Embedding
        self.patch_embed = PatchEmbedding(96, embed_dim)

        # Encoder (Swin Transformer Blocks + Patch Merging)
        self.encoder_layers = nn.ModuleList([
            SwinTransformerBlock(embed_dim, num_heads),
            PatchMerging(embed_dim),
            SwinTransformerBlock(embed_dim * 2, num_heads),
            PatchMerging(embed_dim * 2),
            SwinTransformerBlock(embed_dim * 4, num_heads),
            PatchMerging(embed_dim * 4),
            SwinTransformerBlock(embed_dim * 8, num_heads)
        ])

        # Decoder (Upsample + Skip Connections)
        self.upsample_layers = nn.ModuleList([
            UpsampleLayer(embed_dim * 8),
            UpsampleLayer(embed_dim * 4),
            UpsampleLayer(embed_dim * 2),
            UpsampleLayer(embed_dim)
        ])

        # 输出卷积层
        self.conv2 = nn.Conv2d(embed_dim, 3, kernel_size=3, padding=1)

    def forward(self, x):
        # 预处理：卷积层
        x = self.conv1(x)

        # 变换到嵌入空间
        x = self.patch_embed(x)

        # 编码器
        encoder_outputs = []
        for layer in self.encoder_layers:
            x = layer(x)
            if isinstance(layer, SwinTransformerBlock):
                encoder_outputs.append(x)

        # 解码器
        for idx, upsample_layer in enumerate(self.upsample_layers):
            x = upsample_layer(x)
            if idx < len(encoder_outputs):
                x = x + encoder_outputs[-(idx + 1)]  # Skip connection

        # 恢复到图像空间
        x = self.conv2(x)
        return x

# 示例使用模型
model = ColorRestorationNetwork()
print(model)

```

```py
# 多颜色空间预处理过程
import cv2
import numpy as np

def color_space_preprocessing(image):
    """
    将图像从RGB转换到HSV和Lab颜色空间，并拼接成9通道图像。

    参数:
    - image: 输入图像，形状为 (H, W, 3)，RGB格式。

    返回:
    - concatenated_image: 输出图像，形状为 (H, W, 9)。
    """
    # 将图像从 RGB 转换到 HSV
    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)

    # 将图像从 RGB 转换到 Lab
    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2Lab)

    # 拼接 RGB, HSV 和 Lab 图像
    concatenated_image = np.concatenate((image, hsv_image, lab_image), axis=2)

    return concatenated_image

# 示例用法
if __name__ == "__main__":
    # 读取本地图像，假设图像存储在 'input_image.png' 路径下
    input_image = cv2.imread('input_image.png')
    
    # 转换为 RGB 格式（cv2.imread 默认读取为 BGR）
    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)
    
    # 进行颜色空间预处理
    preprocessed_image = color_space_preprocessing(input_image)
    
    # 输出预处理后的图像形状
    print(f"预处理后的图像形状: {preprocessed_image.shape}")  # 预期输出 (H, W, 9)
    
    # 如果需要，可以将处理后的图像保存为单独的文件
    # cv2.imwrite('preprocessed_image.png', preprocessed_image)


```

```py
# Patch Embedding
# import torch
import torch.nn as nn

class ColorRestorationPreprocessing(nn.Module):
    def __init__(self, embed_dim):
        super(ColorRestorationPreprocessing, self).__init__()
        self.embed_dim = embed_dim
        
        # 第一个卷积层，将通道数从 9 转换为 96
        self.conv = nn.Conv2d(in_channels=9, out_channels=96, kernel_size=3, stride=1, padding=1)
        
        # Patch Embedding 层，通过一个卷积操作进行
        self.patch_embedding = nn.Conv2d(in_channels=96, out_channels=self.embed_dim, kernel_size=4, stride=4, padding=0)
        
        # 可选：增加一个批归一化层和一个激活函数
        self.bn = nn.BatchNorm2d(96)
        self.relu = nn.ReLU()

    def forward(self, x):
        # 通过卷积层将通道数从 9 转换为 96
        x = self.conv(x)
        x = self.bn(x)  # 批归一化
        x = self.relu(x)  # 激活函数

        # 通过 Patch Embedding 层调整维度和尺度
        x = self.patch_embedding(x)
        
        return x

# 示例用法
if __name__ == "__main__":
    # 假设输入图像形状为 (1, 9, 256, 256)，即 (Batch size, Channels, Height, Width)
    input_tensor = torch.randn(1, 9, 256, 256)  # 生成一个随机的输入张量
    
    # 定义模型，假设目标嵌入维度 C 为 128
    model = ColorRestorationPreprocessing(embed_dim=128)
    
    # 运行模型
    output_tensor = model(input_tensor)
    
    # 输出特征图的形状
    print(f"输出特征图的形状: {output_tensor.shape}")  # 预期输出 (1, 128, 64, 64)

```









---------------

```py
#细节提升网络：通过引入残差模块来增强高频图像的细节。 使用 ResNet 块来增强高频分量的细节
class ResNetBlock(nn.Module):
    def __init__(self, in_channels):
        super(ResNetBlock, self).__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(in_channels)
        )

    def forward(self, x):
        return x + self.block(x)

class DetailEnhancementNetwork(nn.Module):
    def __init__(self, in_channels):
        
        super(DetailEnhancementNetwork, self).__init__()
        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)
        
        # 多个 ResNet 块，用于增强高频特征的细节
        self.resnet_blocks = nn.Sequential(
            ResNetBlock(in_channels),
            ResNetBlock(in_channels),
            ResNetBlock(in_channels)
        )

    def forward(self, x):
        x = self.conv(x)
        x = self.resnet_blocks(x)
        return x

```



```py
# 图像重建：将改进后的低频和高频分量通过 IDWT 重建为最终的水下图像
class ImageReconstructionNetwork(nn.Module):
    def __init__(self):
        
        # # 将改进的低频和高频分量通过 IDWT 重建为最终的图像。
        super(ImageReconstructionNetwork, self).__init__()
    
    def forward(self, low_freq, high_freq):
        cA, cH, cV, cD = high_freq
        return idwt(low_freq, cH, cV, cD)

```



```py
# 鉴别器网络：用于判别生成的图像和参考图像的真伪
class Discriminator(nn.Module):
    def __init__(self, in_channels):
        
        # Discriminator：一个基于卷积层的网络，用于判别输入图像是真实的还是生成的
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

```



```py
# 混合损失函数：优化网络的训练过程
```



```py
# 总体调用
class UnderwaterImageRestoration(nn.Module):
    def __init__(self, in_channels, num_heads):
        super(UnderwaterImageRestoration, self).__init__()
        self.dwt = dwt
        self.idwt = idwt
        self.color_restoration = ColorRestorationNetwork(in_channels, num_heads)
        self.detail_enhancement = DetailEnhancementNetwork(in_channels)
        self.reconstruction = ImageReconstructionNetwork()
        self.discriminator = Discriminator(in_channels)

    def forward(self, x):
        # 离散小波变换
        low_freq, cH, cV, cD = self.dwt(x)
        
        # 颜色还原网络
        improved_low_freq = self.color_restoration(low_freq)
        
        # 细节提升网络
        high_freq = (cH, cV, cD)
        improved_high_freq = self.detail_enhancement(high_freq)
        
        # 图像重建
        reconstructed_image = self.reconstruction(improved_low_freq, improved_high_freq)
        
        return reconstructed_image

```



1. 训练模型代码1

```cpp
import torch.optim as optim

# 定义损失函数
adversarial_loss = nn.BCELoss()
reconstruction_loss = nn.L1Loss()

# 初始化模型和优化器
model = UnderwaterImageRestoration(in_channels, num_heads).cuda()
discriminator = Discriminator(in_channels).cuda()

optimizer_G = optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

# 假设我们有 DataLoader 提供的训练数据
for epoch in range(num_epochs):
    for i, data in enumerate(dataloader):
        imgs, _ = data
        imgs = imgs.cuda()

        # 生成真实和虚假标签
        valid = torch.ones((imgs.size(0), 1), requires_grad=False).cuda()
        fake = torch.zeros((imgs.size(0), 1), requires_grad=False).cuda()

        # -----------------
        # 训练生成器
        # -----------------
        optimizer_G.zero_grad()

        restored_imgs = model(imgs)

        g_loss = adversarial_loss(discriminator(restored_imgs), valid) + reconstruction_loss(restored_imgs, imgs)

        g_loss.backward()
        optimizer_G.step()

        # ---------------------
        # 训练鉴别器
        # ---------------------
        optimizer_D.zero_grad()

        real_loss = adversarial_loss(discriminator(imgs), valid)
        fake_loss = adversarial_loss(discriminator(restored_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()
        optimizer_D.step()

        print(f"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]")

```

**adversarial_loss**：对抗性损失，用于判别真假图像的分类损失。

**reconstruction_loss**：重建损失，用于衡量生成图像与原始图像的相似性（L1 损失）。

**optimizer_G 和 optimizer_D**：用于优化生成器和鉴别器的 Adam 优化器。

**valid 和 fake**：用于标签的真实和虚假标签。

**g_loss**：生成器的损失，包括对抗性损失和重建损失。

**d_loss**：鉴别器的损失，包括真实图像和生成图像的损失。

----------

2. 训练模型代码2

```py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
from tqdm import tqdm

# 假设定义了 Swin Transformer 和 ResNet 的相关模块
# 请确保在前面我们定义了 `UnderwaterImageRestoration` 类

class UnderwaterDataset(Dataset):
    def __init__(self, underwater_image_dir, ground_truth_image_dir, transform=None):
        self.underwater_image_dir = underwater_image_dir
        self.ground_truth_image_dir = ground_truth_image_dir
        self.transform = transform
        self.underwater_images = os.listdir(underwater_image_dir)
        self.ground_truth_images = os.listdir(ground_truth_image_dir)

    def __len__(self):
        return len(self.underwater_images)

    def __getitem__(self, idx):
        underwater_image_path = os.path.join(self.underwater_image_dir, self.underwater_images[idx])
        ground_truth_image_path = os.path.join(self.ground_truth_image_dir, self.ground_truth_images[idx])

        underwater_image = Image.open(underwater_image_path).convert('RGB')
        ground_truth_image = Image.open(ground_truth_image_path).convert('RGB')

        if self.transform:
            underwater_image = self.transform(underwater_image)
            ground_truth_image = self.transform(ground_truth_image)

        return underwater_image, ground_truth_image

# 数据预处理和加载
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

train_dataset = UnderwaterDataset(
    underwater_image_dir="path/to/train/underwater_images",
    ground_truth_image_dir="path/to/train/ground_truth_images",
    transform=transform
)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)

# 验证集加载
val_dataset = UnderwaterDataset(
    underwater_image_dir="path/to/val/underwater_images",
    ground_truth_image_dir="path/to/val/ground_truth_images",
    transform=transform
)

val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)

# 初始化模型
model = UnderwaterImageRestoration(in_channels=3, num_heads=8).cuda()

# 定义损失函数
mse_loss = nn.MSELoss()  # 恢复图像和地面真值之间的像素级误差
adversarial_loss = nn.BCEWithLogitsLoss()  # 对抗性损失，用于区分真假图像

# 定义优化器
optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.5, 0.999))

# 训练循环
num_epochs = 100

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    
    for i, (underwater_image, ground_truth_image) in enumerate(tqdm(train_loader)):
        underwater_image = underwater_image.cuda()
        ground_truth_image = ground_truth_image.cuda()
        
        optimizer.zero_grad()
        
        restored_image = model(underwater_image)
        
        # 计算恢复损失
        loss = mse_loss(restored_image, ground_truth_image)
        
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    
    avg_loss = running_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
    
    # 验证过程
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for i, (underwater_image, ground_truth_image) in enumerate(val_loader):
            underwater_image = underwater_image.cuda()
            ground_truth_image = ground_truth_image.cuda()
            
            restored_image = model(underwater_image)
            
            loss = mse_loss(restored_image, ground_truth_image)
            val_loss += loss.item()
    
    avg_val_loss = val_loss / len(val_loader)
    print(f"Validation Loss: {avg_val_loss:.4f}")
    
    # 每个 epoch 保存模型
    torch.save(model.state_dict(), f"model_epoch_{epoch+1}.pth")

print("Training complete.")

# 训练后的模型将会被保存为 'model_epoch_{epoch+1}.pth'

```

### 代码详细解释

1. **数据集类** (`UnderwaterDataset`):
   - 该类继承自 `torch.utils.data.Dataset`，用于加载水下图像及其相应的地面真值图像。
   - `__getitem__` 方法将返回一个元组 `(underwater_image, ground_truth_image)`。
2. **数据预处理和加载**:
   - 使用 `transforms` 对图像进行大小调整和归一化。
   - `DataLoader` 用于将数据分批加载，以加快训练过程。
3. **模型初始化**:
   - `UnderwaterImageRestoration` 类用于定义我们的水下图像恢复模型。
   - 将模型移动到 GPU 上。
4. **损失函数**:
   - 使用均方误差损失（`MSELoss`）来评估恢复图像和地面真值之间的像素级误差。
   - 对抗性损失（`BCEWithLogitsLoss`）用于鉴别器判断生成的图像是真是假的能力。
5. **优化器**:
   - 使用 `Adam` 优化器来更新模型的权重。
6. **训练循环**:
   - 遍历训练集，计算损失，更新模型权重。
   - 通过验证集评估模型性能，并在每个 epoch 结束时保存模型权重。

运行说明

1. **路径替换**:
   - 替换 `path/to/train/underwater_images` 和 `path/to/train/ground_truth_images` 为实际的训练数据集路径。
   - 替换 `path/to/val/underwater_images` 和 `path/to/val/ground_truth_images` 为实际的验证数据集路径。
2. **训练环境**:
   - 确保在具有 GPU 支持的机器上运行，以加快训练过程。
   - 安装必要的 Python 包，如 `torch`, `torchvision`, `PIL`, `tqdm`。
3. **模型保存**:
   - 训练期间，模型在每个 epoch 后会保存一次，文件名为 `model_epoch_{epoch+1}.pth`。

通过这些步骤，你应该能够从本地数据集训练水下图像恢复模型。
